---
title: "Practical Machine Learning course project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(caret)
require(formattable)
require(tictoc)

## clean up
rm(list=ls())

set.seed(12345)
```
  
### **Background**  
  
Taken from: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har  
  
In this work (see the paper) we first define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. We tried out an on-body sensing approach (dataset here), but also an "ambient sensing approach" (by using Microsoft Kinect - dataset still unavailable)

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:  
* exactly according to the specification (Class A),  

* throwing the elbows to the front (Class B),  

* lifting the dumbbell only halfway (Class C),  

* lowering the dumbbell only halfway (Class D)  

* and throwing the hips to the front (Class E).  



Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).  


  
### **Objective**  
  
* Predict the manner in which subjects exercise (denoted by the *classe* variable in the dataset).  

  
* Estimate the out of sample error rate.  
  
  
  
### **Assumptions**  
  
* 99% accuracy is required (as per course guidelines)  

* Model accuracy is more important than interpretability/explainability.  
  
* Time taken to create model is not important - a computationally-intensive solution is acceptable.  
  
### **Approach**  
  
* Data exploration and preprocessing:  

  * Check whether outcomes are balanced in dataset.  

  * Exclude features with many values missing.  

* Split training set into training and validation sets  

* Model building:  
  
  * Train random forest model to predict *classe* using training set  

* Model evaluation:  
  
  * Preprocess validation set in same way as training set  
  
  * Use trained model to make predictions on validation set  

  * Estimate out of sample accuracy based on predictions against validation set  
  
* Prediction on test set (once only):  

** Load test data  

** Preprocess test data in same way as training data  

** Make predictions on test data using model  



### **Load data**  
  
  
```{r load training set}
training <- read.csv(file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",stringsAsFactors = TRUE,na.strings=c("","NA"))

```
  
We have many more observations in the training set (`r nrow(training)`) than features (`r ncol(training)-1`).  
  
We have sufficient observations that we can use a subset of the training set as a "validation set" that we can use to get estimates of out-of-sample error.  
  
We have a mix of identifier and measurement variables - we will omit the identifier variables (columns 1-7, inclusive.  
  
```{r omit identifier variables}
training <- training[ , -c(1:7)]

```

  
Many columns have many missing values - the distribution of proportion of missing values is very binary - columns either have no missing values, or almost all missing values.  
  
We will omit columns with >90% of values missing.  

```{r omit columns with many missing values}

## what proportion of training set is NA?
sum(is.na(training))/(sum(!is.na(training))+sum(is.na(training)))

## what is the distribution of proportion of missing values?
qplot(colMeans(is.na(training)))

training <- training[ ,which(colMeans(!is.na(training)) > 0.9)]

dim(training[complete.cases(training), ])

```

  
Now we have data with no missing values, no imputation required.  
  
Outcomes (*classe*) are well balanced across observations in the training set.  
  
  
```{r explore training data}

qplot(training$classe)  

```
  
*classe* values are slightly skewed towards "A", but we will consider the data to be sufficiently balanced that we can take a random sample when creating the validation set.  
  
  

### **Split training data into training and validation sets**  
  
  
```{r split training data into training and validation sets}

## check dimensions of training set
dim(training)

## split into 70% that will stay in training, 30% will go into validation
inTrain <- createDataPartition(y=training$classe,p=0.70, list = FALSE) 

## create the validation set
validation <- training[-inTrain,]		##create a validation set
## check dimensions
dim(validation)

## now subset the training set
training <- training[inTrain,]		      ##subset the training set
## check dimensions
dim(training)

```
  

### **Train prediction model**  
  
We will create random forest and boosted tree models.  


  
```{r build models}
## set up training control options for repeated cross-validation ...
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)

tic("fit rf model to training_PC")
modFit_rf <- train(classe ~ ., data=training, method="rf", trControl = fit.control, allowParallel = TRUE)
toc()

```

  
### **Check accuracy against validation set**  
  
Make predictions against validation set and estimate (out-of-sample) accuracy for model.  

  
```{r predict against validation set and measure accuracy}

## make predictions on validation set 
pred_rf <- predict(modFit_rf, validation)
confusionMatrix(pred_rf, validation$classe)$overall


## store out of sample accuracy
out_sample_accuracy <- confusionMatrix(pred_rf, validation$classe)$overall[['Accuracy']]

```




### **Load test set**  
  
```{r load test set}

test <- read.csv(file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",stringsAsFactors = FALSE,na.strings=c("","NA"))

dim(test)

unique(test$user_name)
"classe" %in% names(test)
```
  
### **Preprocess test set**  


```{r preprocess test set to match training and validation sets}

## omit the same columns from test 
test <- test[ ,which(names(test) %in% names(training))]

```



### **Make predictions against test set using trained model**  
  
Based on the validation set, the **expected out of sample accuracy for the stacked model is `r round(100*out_sample_accuracy, digits = 1)`%**  
  
    
Predicted *classe* values are:  

  
  
```{r make final predictions against test set}
Pred_test <- predict(modFit_rf, test)

formattable(data.frame("ID" = seq_along(Pred_test), "Predicted_classe" = Pred_test))


```
  



  
  
  
  